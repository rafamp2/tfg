DEV_MODE: user ##user or dev, cambia la autorizacion sobre cosas que se pueden hacer

# Memoria sobre la que se cargarn los modelos
# cuda --> VRAM
# cpu --> RAM
LLM_DEVICE: cuda
TTS_DEVICE: cuda
DEVICE: cpu


MODELS_DIR: ./models

EMBEDDINGS_PATH: multilingual-e5-large-instruct

VECTORDB_TYPE: chroma
VECTORDB_PATH: ./vectordb/chroma

TTS_PATH: src/tmp

# Los wrappers son: TRANSFORMERS, LLAMACPP y CTRANSFORMERS
WRAPPER: LLAMACPP

# Carpeta al LLM
LLM_PATH: gemma-2-9b-it-GGUF

# Para modelos que no sean una carpeta, sino un solo archivo (.gguf especialmente) 
# hay que especificar el archivo (y tambi√©n la carpeta en el LLM_PATH)
LLM_NAME: gemma-2-9b-it-Q6_K.gguf 

LLM_CONFIG:
  MAX_NEW_TOKENS: 1024
  TEMPERATURE: 0.7
  TOP_P: 0.8
  REPETITION_PENALTY: 1.1
  CONTEXT_LENGTH: 4096
  N_GPU_LAYERS: 0
  N_THREADS: 1
  ADD_PROMPT: False

BASE_RETRIEVER_CONFIG:
  SEARCH_K: 4

RERANKER_PATH: bge-reranker-base
RERANKER_TYPE: bge

RERANK_RETRIEVER_CONFIG:
  SEARCH_K: 10
  TOP_N: 4

COMPRESSION_RETRIEVER_CONFIG:
  SEARCH_K: 10
  SIMILARITY_THRESHOLD: 0.5

TTS_MODEL: XTTS-v2
TTS_CONFIG: config.json
WHISPER_MODEL: medium
AUDIO_CONFIG:
  AUDIO_DIR: ./audio
  VOICE_FILE: voice_style.wav
  GPT_COND_LEN: 5
  

TEXT_SPLIT_MODE: default  
CHUNK_SIZE: 500
CHUNK_OVERLAP: 50
SEPARATORS: ["\n\n", "\n", ". ", " ", ""]

